{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import precision_score\n",
    "    \n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the data\n",
    "data = pd.read_csv('AggredgatedData.csv', sep=',', na_values=[\" \", \"\"], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(data.columns[1:-1])\n",
    "X = data[features]\n",
    "Y = data['2016ODabovenatavg']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##I. Model Selection Using Grid Search and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a useful tool for parameter tuning, and examining which combinations of parameters yield the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "classifiers = []\n",
    "search_grid = [{'penalty': ['l1', 'l2'],\n",
    "                 'C': [0.01, 0.1, 0.5, 1, 5]\n",
    "                },\n",
    "                {'n_estimators': [10, 50, 60, 100],\n",
    "                 'criterion': ['gini', 'entropy'],\n",
    "                 'max_features': ['sqrt','log2', None],\n",
    "                },\n",
    "                {'penalty': ['l1', 'l2'],\n",
    "                 'loss': ['hinge', 'log', 'modified_huber'],\n",
    "                 'tol': [1e-3, 1e-5, 1e-7]\n",
    "                 },\n",
    "                {'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                 'C': [0.1, 1, 10],\n",
    "                 'gamma': [0.01, 0.1, 0.5, 1, 5, 10]} ]\n",
    "\n",
    "\n",
    "classifiers.append(GridSearchCV(LogisticRegression(), search_grid[0], cv=5, verbose=0))\n",
    "\n",
    "classifiers.append(GridSearchCV(RandomForestClassifier(), search_grid[1], cv=5, verbose=0))\n",
    "\n",
    "classifiers.append(GridSearchCV(SGDClassifier(), search_grid[2], cv=5, verbose=0))\n",
    "\n",
    "classifiers.append(GridSearchCV(SVC(), search_grid[3], cv=5, verbose=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 0.1}\n",
      "{'max_features': None, 'n_estimators': 10, 'criterion': 'gini'}\n",
      "{'penalty': 'l2', 'loss': 'log', 'tol': 1e-07}\n",
      "{'kernel': 'linear', 'C': 10, 'gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classifiers)):\n",
    "    classifiers[i].fit(X_train, Y_train)\n",
    "    print(classifiers[i].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(classifiers)):\n",
    "    classifiers[i].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.660377358491\n",
      "0.962264150943\n",
      "0.584905660377\n",
      "0.622641509434\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classifiers)):\n",
    "    print(classifiers[i].score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search:\n",
    "  * Logistic Regression: 0.660377358491\n",
    "  * Random Forest: 0.962264150943\n",
    "  * Stochastic Gradient Descent: 0.622641509434\n",
    "  * SVM: 0.622641509434\n",
    "  \n",
    "Overall, Random Forest shows best performance regardless of which search method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##II. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros: \n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "Cons:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cross Validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. \n",
    "\n",
    "This situation is called overfitting. \n",
    "\n",
    "To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. \n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. \n",
    "\n",
    "This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n",
    "\n",
    "To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "    * A model is trained using k-1 of the folds as training data;\n",
    "    * the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "    \n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear kernel = Straight Line (hyperplane) as the decision boundary\n",
    "* rarely used in practice\n",
    "\n",
    "RBF = commonly used kernel in SVC\n",
    "2 parameters:\n",
    "* gamma\n",
    "* C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma:\n",
    "*  'spread' of the kernel and therefore the decision region.\n",
    "* low gamma -> the 'curve' of the decision boundary is very low and thus the decision region is very broad (underfitting)\n",
    "* gamma = 10 (The decision boundary starts to be highly effected by individual data points (i.e. variance)).\n",
    "* high gamma -> the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points (overfitting)\n",
    "* If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "C:\n",
    "* penalty for misclassifying a data point\n",
    "* small C -> classifier is okay with misclassified data points (high bias, low variance)\n",
    "* big C -> classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance)\n",
    "\n",
    "C > 10 is too slow\n",
    "\n",
    "degree : int, optional (default=3)\n",
    "\n",
    "* Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear | Gamma: 0.01 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 0.01 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 1.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 10.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 1\n",
      "[ 0.70588235  0.6875      0.5         0.75        0.8125      0.5625      0.875\n",
      "  0.625       0.8         0.8       ]\n",
      "Accuracy: 0.71 (+/- 0.23)\n",
      "Kernel: linear | Gamma: 100.00 | C: 10\n",
      "[ 0.52941176  0.8125      0.5625      0.625       0.75        0.6875      0.875\n",
      "  0.625       0.6         0.53333333]\n",
      "Accuracy: 0.66 (+/- 0.23)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 1\n",
      "[ 0.76470588  0.6875      0.5625      0.6875      0.875       0.6875\n",
      "  0.6875      0.6875      0.6         0.8       ]\n",
      "Accuracy: 0.70 (+/- 0.17)\n",
      "Kernel: rbf | Gamma: 0.01 | C: 10\n",
      "[ 0.70588235  0.75        0.4375      0.625       0.875       0.6875\n",
      "  0.8125      0.75        0.66666667  0.66666667]\n",
      "Accuracy: 0.70 (+/- 0.22)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 1.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 10.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 1\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: rbf | Gamma: 100.00 | C: 10\n",
      "[ 0.64705882  0.625       0.625       0.625       0.625       0.625       0.625\n",
      "  0.625       0.66666667  0.66666667]\n",
      "Accuracy: 0.64 (+/- 0.03)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 1\n",
      "[ 0.82352941  0.625       0.5625      0.6875      0.5625      0.5625\n",
      "  0.5625      0.6875      0.73333333  0.6       ]\n",
      "Accuracy: 0.64 (+/- 0.17)\n",
      "Kernel: sigmoid | Gamma: 0.01 | C: 10\n",
      "[ 0.76470588  0.5625      0.5625      0.625       0.5         0.5625\n",
      "  0.5625      0.625       0.73333333  0.53333333]\n",
      "Accuracy: 0.60 (+/- 0.16)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 1\n",
      "[ 0.76470588  0.5625      0.5         0.625       0.8125      0.625       0.8125\n",
      "  0.6875      0.73333333  0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.20)\n",
      "Kernel: sigmoid | Gamma: 1.00 | C: 10\n",
      "[ 0.82352941  0.5         0.4375      0.625       0.75        0.625       0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5625      0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.68 (+/- 0.22)\n",
      "Kernel: sigmoid | Gamma: 10.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625      0.75\n",
      "  0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.67 (+/- 0.24)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 1\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.66666667]\n",
      "Accuracy: 0.67 (+/- 0.23)\n",
      "Kernel: sigmoid | Gamma: 100.00 | C: 10\n",
      "[ 0.82352941  0.5         0.5         0.625       0.8125      0.5625\n",
      "  0.6875      0.6875      0.8         0.6       ]\n",
      "Accuracy: 0.66 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "kernels = ['linear', 'rbf', 'sigmoid']\n",
    "\n",
    "for i in kernels:\n",
    "    for j, C in enumerate((0.01, 1, 10, 100)):\n",
    "        for k, D in enumerate((1, 10)):\n",
    "            clf = SVC(C=D, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                      decision_function_shape='ovr', degree=3, gamma=C, kernel=i,\n",
    "                      max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                      tol=0.001, verbose=False)\n",
    "            clf.fit(X_train, Y_train) \n",
    "            scores = cross_val_score(clf, X, Y, cv = 10)\n",
    "\n",
    "            print (\"Kernel: %s | Gamma: %0.2f | C: %i\" % (i, C, D))\n",
    "            print scores\n",
    "            print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal Model\n",
    "\n",
    "Kernel: rbf | Gamma: 0.01 | C: 1\n",
    "\n",
    "[ 0.76470588  0.6875      0.5625      0.6875      0.875       0.6875\n",
    "  0.6875      0.6875      0.6         0.8       ]\n",
    "  \n",
    "Accuracy: 0.70 (+/- 0.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf0 = SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape='ovr', degree=3, gamma=0.01, kernel='linear',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "clf0.fit(X_train, Y_train) \n",
    "scores = cross_val_score(clf0, X, Y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.63848801e-01,  -6.68540451e-02,   3.09387286e-01,\n",
       "         -2.05750837e-01,   1.34634014e+00,   1.40482994e-01,\n",
       "          7.49506633e-01,   1.59324072e+00,   3.68813954e-01,\n",
       "          3.97217579e-02,  -3.60074262e-01,   1.83569736e-01,\n",
       "         -2.72166587e-01,  -2.27081181e-01,   8.76271007e-03,\n",
       "          3.60658592e-01,  -4.52500113e-01,  -2.06844360e-01,\n",
       "          2.52252443e-02,   3.51946070e-01,  -8.41550902e-01,\n",
       "          5.08848354e-01,  -2.47331720e-01,   7.35406646e-01,\n",
       "         -1.60632132e-02,  -1.61889112e-01,   5.63491887e-01,\n",
       "         -3.35239154e-01,   8.46799623e-04,   2.23128289e-01,\n",
       "          5.84754921e-02,  -2.64266052e-01,   1.39304776e-01,\n",
       "          1.76294694e-02,  -5.67709251e-02,  -7.26662105e-01,\n",
       "          1.72026303e-03,  -1.46720138e-02,   5.96602027e-01,\n",
       "          2.40489622e-03]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf0.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.91970688])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf0.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##II. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary classifier\n",
    "\n",
    "L1 regularization (also called least absolute deviations) \n",
    "* push feature coefficients to 0, creating a method for feature selection. \n",
    "* as C decreases, more coefficients become 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False  True False  True  True False False  True  True False\n",
      "  True False  True  True  True  True False False  True  True False False\n",
      " False False  True  True  True False  True False  True False  True False\n",
      "  True False False False False False False False  True False  True False\n",
      "  True False  True  True False]\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(penalty='l1', dual=False, tol=0.01, C=100.0, fit_intercept=True, \n",
    "                   intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', \n",
    "                   max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "clf1.fit(X_train, Y_train) \n",
    "\n",
    "print(clf1.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.64705882  0.6875      0.5         0.625       0.6875      0.6875      0.875\n",
      "  0.5625      0.6         0.73333333]\n",
      "Accuracy: 0.66 (+/- 0.19)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf1, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=100.00\n",
      "Sparsity with L1 penalty: 10.00%\n",
      "score with L1 penalty: 0.8491\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.8491\n",
      "C=1.00\n",
      "Sparsity with L1 penalty: 57.50%\n",
      "score with L1 penalty: 0.7736\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7925\n",
      "C=0.01\n",
      "Sparsity with L1 penalty: 95.00%\n",
      "score with L1 penalty: 0.6415\n",
      "Sparsity with L2 penalty: 0.00%\n",
      "score with L2 penalty: 0.7421\n"
     ]
    }
   ],
   "source": [
    "#Comparison of the sparsity (% of zero coefficients) of solutions when L1 and L2 penalty \n",
    "#are used for different values of C. \n",
    "#We can see that large values of C give more freedom to the model. \n",
    "#Conversely, smaller values of C constrain the model more. \n",
    "#In the L1 penalty case, this leads to sparser solutions.\n",
    "\n",
    "for i, C in enumerate((100, 1, 0.01)):\n",
    "    # turn down tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    clf_l1_LR.fit(X, Y)\n",
    "    clf_l2_LR.fit(X, Y)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "\n",
    "    print(\"C=%.2f\" % C)\n",
    "    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n",
    "    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, Y))\n",
    "    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n",
    "    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "* Efficiency.\n",
    "* Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "Cons:\n",
    "* requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "* sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
    "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
    "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
    "       shuffle=True, tol=None, verbose=0, warm_start=False)\n",
    "\n",
    "clf3.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True,  True,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False, False,  True, False, False, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True, False,  True,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -6.54022237e+00,  -3.27011118e+01,   1.30804447e+01,\n",
       "         -3.27011118e+01,   1.37344670e+02,   6.54022237e+01,\n",
       "          3.03462829e+02,   2.92448884e+02,   2.79536244e+02,\n",
       "          5.59693573e+00,  -3.35869292e+00,   4.68472358e+00,\n",
       "         -2.40401076e+01,   1.37421322e+01,  -1.07683394e+02,\n",
       "          5.14925875e+01,   2.28010661e+01,  -3.71727784e+03,\n",
       "          7.59091781e+00,   2.46139876e+01,   8.68910631e+00,\n",
       "         -1.23842897e+01,   1.28433354e+01,   2.46470546e+01,\n",
       "         -1.94903944e+01,  -1.67556318e+01,   1.30093645e+01,\n",
       "         -2.82697697e+01,   3.54515140e+01,   5.42878455e+01,\n",
       "          3.97549208e+01,  -1.64648076e+01,   5.47546539e+01,\n",
       "          2.79568345e-01,  -8.59385219e-01,  -3.72858077e+01,\n",
       "         -2.85586723e-01,  -7.69634455e-02,   7.73381295e+01,\n",
       "          8.47116137e-02]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.71209387])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3.intercept_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70588235  0.75        0.5625      0.625       0.5         0.625       0.6875\n",
      "  0.6875      0.53333333  0.8       ]\n",
      "Accuracy: 0.65 (+/- 0.18)\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf3, X, Y, cv = 10)\n",
    "print scores\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##IV. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n",
    "\n",
    "Pros:\n",
    "\n",
    "* It runs efficiently on large data bases.\n",
    "* It can handle thousands of input variables without variable deletion.\n",
    "* It gives estimates of what variables are important in the classification.\n",
    "* It generates an internal unbiased estimate of the generalization error as the forest building progresses.\n",
    "* It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.\n",
    "* It has methods for balancing error in class population unbalanced data sets.\n",
    "* Generated forests can be saved for future use on other data.\n",
    "* Prototypes are computed that give information about the relation between the variables and the classification.\n",
    "* It computes proximities between pairs of cases that can be used in clustering, locating outliers, or (by scaling) give interesting views of the data.\n",
    "* The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection.\n",
    "* It offers an experimental method for detecting variable interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Trees in RF\n",
    "\n",
    "\n",
    "Random forests does not overfit. You can run as many trees as you want. It is fast. Running on a data set with 50,000 cases and 100 variables, it produced 100 trees in 11 minutes on a 800Mhz machine. For large data sets the major memory requirement is the storage of the data itself, and three integer arrays with the same dimensions as the data. If proximities are calculated, storage requirements grow as the number of cases times the number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n",
    "\n",
    "Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.\n",
    "\n",
    "* https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks\n",
    "\n",
    "The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations z_i = (x_i, y_i). \n",
    "\n",
    "The out-of-bag (OOB) error is the average error for each z_i calculated using predictions from the trees that do not contain z_i in their respective bootstrap sample. \n",
    "\n",
    "This allows the RandomForestClassifier to be fit and validated whilst being trained. \n",
    "\n",
    "* T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, p592-593, Springer, 2009."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = RandomForestClassifier(n_estimators=50, criterion='entropy', max_depth=None, \n",
    "                              min_samples_split=2, \n",
    "                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                              max_features='auto', \n",
    "                       max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                       bootstrap=True, oob_score=True, n_jobs=1, random_state=None, verbose=0, \n",
    "                       warm_start=False, class_weight=None)\n",
    "            \n",
    "clf4.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90566037735849059"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Figuring out ok counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = clf4.predict(X) \n",
    "itemindex = np.where(preds==False)\n",
    "ok_counties = data.ix[itemindex]\n",
    "index_ok_counties = ok_counties.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2017 = pd.read_csv('AggregatedData2017.csv', sep=',', na_values=[\" \", \"\"], index_col=0)\n",
    "data2018 = pd.read_csv('AggregatedData2018.csv', sep=',', na_values=[\" \", \"\"], index_col=0)\n",
    "\n",
    "ok_counties_2017 = data2017.ix[index_ok_counties]\n",
    "ok_counties_2018 = data2018.ix[index_ok_counties]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(ok_counties_2017.columns[1:-1])\n",
    "X = ok_counties_2017[features]\n",
    "preds = clf4.predict(X) \n",
    "itemindex = np.where(preds==True)\n",
    "risky_counties = data2017.ix[itemindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Rural</th>\n",
       "      <th>Rehab_Count</th>\n",
       "      <th>EMS_COUNT</th>\n",
       "      <th>hospital_count</th>\n",
       "      <th>dropbox_247availability</th>\n",
       "      <th>dropbox_count</th>\n",
       "      <th>1yrlag</th>\n",
       "      <th>5yrlag</th>\n",
       "      <th>10yrlag</th>\n",
       "      <th>...</th>\n",
       "      <th>ER_5yrlag</th>\n",
       "      <th>ER_10yrlag</th>\n",
       "      <th>drug_ER_Visits_2017</th>\n",
       "      <th>teenage_pregnancy_2017</th>\n",
       "      <th>2017_children_poverty</th>\n",
       "      <th>2017_death_rate</th>\n",
       "      <th>2017_birth_rate</th>\n",
       "      <th>2017_prescription</th>\n",
       "      <th>2017_ODRate</th>\n",
       "      <th>2017ODabovenatavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Banks</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.076373</td>\n",
       "      <td>-0.261098</td>\n",
       "      <td>0.231503</td>\n",
       "      <td>...</td>\n",
       "      <td>8.221374</td>\n",
       "      <td>16.660819</td>\n",
       "      <td>0.006040</td>\n",
       "      <td>0.011871</td>\n",
       "      <td>0.213006</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.011875</td>\n",
       "      <td>0.386764</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.283471</td>\n",
       "      <td>1.727376</td>\n",
       "      <td>...</td>\n",
       "      <td>13.701365</td>\n",
       "      <td>17.329787</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.193598</td>\n",
       "      <td>0.008704</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>0.990520</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bleckley</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.772826</td>\n",
       "      <td>0.772826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.726742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.333089</td>\n",
       "      <td>0.009503</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>1.292364</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bryan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474832</td>\n",
       "      <td>0.474832</td>\n",
       "      <td>...</td>\n",
       "      <td>10.824047</td>\n",
       "      <td>11.679245</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.005259</td>\n",
       "      <td>0.163188</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.898534</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bulloch</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.188242</td>\n",
       "      <td>-0.323535</td>\n",
       "      <td>0.352931</td>\n",
       "      <td>...</td>\n",
       "      <td>9.737864</td>\n",
       "      <td>8.102881</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.010492</td>\n",
       "      <td>0.336884</td>\n",
       "      <td>0.007535</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.743703</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Camden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.201572</td>\n",
       "      <td>0.401834</td>\n",
       "      <td>1.102751</td>\n",
       "      <td>...</td>\n",
       "      <td>7.168705</td>\n",
       "      <td>9.075520</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.006585</td>\n",
       "      <td>0.014188</td>\n",
       "      <td>0.701366</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Candler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.119287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119287</td>\n",
       "      <td>...</td>\n",
       "      <td>10.980398</td>\n",
       "      <td>10.878870</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.441832</td>\n",
       "      <td>0.011954</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>1.788549</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000454</td>\n",
       "      <td>0.749205</td>\n",
       "      <td>0.166136</td>\n",
       "      <td>...</td>\n",
       "      <td>14.424508</td>\n",
       "      <td>18.689944</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.015083</td>\n",
       "      <td>0.303207</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>1.107233</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Charlton</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.365728</td>\n",
       "      <td>1.365728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.643264</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.014538</td>\n",
       "      <td>0.403633</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>0.009184</td>\n",
       "      <td>1.055230</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chatham</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.065380</td>\n",
       "      <td>0.168274</td>\n",
       "      <td>0.308467</td>\n",
       "      <td>...</td>\n",
       "      <td>12.468410</td>\n",
       "      <td>10.533582</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.010271</td>\n",
       "      <td>0.261827</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.013927</td>\n",
       "      <td>0.816522</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Clay</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.486449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.486449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.521235</td>\n",
       "      <td>0.011616</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.387811</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Clayton</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053675</td>\n",
       "      <td>-0.055326</td>\n",
       "      <td>0.521975</td>\n",
       "      <td>...</td>\n",
       "      <td>13.829694</td>\n",
       "      <td>23.608696</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>0.369619</td>\n",
       "      <td>0.006306</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.412495</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Crawford</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.498952</td>\n",
       "      <td>-0.248428</td>\n",
       "      <td>-0.248428</td>\n",
       "      <td>...</td>\n",
       "      <td>14.703782</td>\n",
       "      <td>18.265464</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.281088</td>\n",
       "      <td>0.009680</td>\n",
       "      <td>0.008791</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Crisp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.330353</td>\n",
       "      <td>-0.776784</td>\n",
       "      <td>-0.776784</td>\n",
       "      <td>...</td>\n",
       "      <td>19.412979</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>0.012496</td>\n",
       "      <td>0.472186</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>1.724571</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Dodge</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.231370</td>\n",
       "      <td>-0.231370</td>\n",
       "      <td>-0.385096</td>\n",
       "      <td>...</td>\n",
       "      <td>4.494344</td>\n",
       "      <td>11.026898</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.017981</td>\n",
       "      <td>0.387121</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.009953</td>\n",
       "      <td>0.870092</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Elbert</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.513558</td>\n",
       "      <td>2.027116</td>\n",
       "      <td>0.513558</td>\n",
       "      <td>...</td>\n",
       "      <td>8.326039</td>\n",
       "      <td>3.380267</td>\n",
       "      <td>0.004262</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.357183</td>\n",
       "      <td>0.013320</td>\n",
       "      <td>0.010730</td>\n",
       "      <td>1.475771</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Fayette</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.435472</td>\n",
       "      <td>1.870943</td>\n",
       "      <td>0.435472</td>\n",
       "      <td>...</td>\n",
       "      <td>13.708155</td>\n",
       "      <td>15.319048</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.101205</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.947549</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Floyd</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.115796</td>\n",
       "      <td>-0.115796</td>\n",
       "      <td>-0.167808</td>\n",
       "      <td>...</td>\n",
       "      <td>10.740385</td>\n",
       "      <td>11.943463</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.017102</td>\n",
       "      <td>0.321287</td>\n",
       "      <td>0.011752</td>\n",
       "      <td>0.011863</td>\n",
       "      <td>1.484394</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Forsyth</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.540807</td>\n",
       "      <td>1.214909</td>\n",
       "      <td>0.968808</td>\n",
       "      <td>...</td>\n",
       "      <td>9.804511</td>\n",
       "      <td>12.024169</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.069888</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>0.479655</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.593248</td>\n",
       "      <td>9.593248</td>\n",
       "      <td>0.006589</td>\n",
       "      <td>0.028099</td>\n",
       "      <td>0.387093</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>0.007573</td>\n",
       "      <td>0.540507</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.480143</td>\n",
       "      <td>0.559572</td>\n",
       "      <td>3.678716</td>\n",
       "      <td>...</td>\n",
       "      <td>2.154548</td>\n",
       "      <td>1.512713</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.122432</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.084966</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Lamar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287262</td>\n",
       "      <td>0.930893</td>\n",
       "      <td>0.930893</td>\n",
       "      <td>...</td>\n",
       "      <td>14.010309</td>\n",
       "      <td>13.239609</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.008595</td>\n",
       "      <td>0.287860</td>\n",
       "      <td>0.011793</td>\n",
       "      <td>0.010849</td>\n",
       "      <td>0.735635</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Lowndes</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.073608</td>\n",
       "      <td>0.455760</td>\n",
       "      <td>2.396772</td>\n",
       "      <td>...</td>\n",
       "      <td>11.632353</td>\n",
       "      <td>20.802030</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.012464</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.014248</td>\n",
       "      <td>0.891325</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      County  Rural  Rehab_Count  EMS_COUNT  hospital_count  \\\n",
       "5      Banks      1            0          1               0   \n",
       "7     Bartow      0            1          1               1   \n",
       "11  Bleckley      1            1          1               1   \n",
       "14     Bryan      1            0          1               0   \n",
       "15   Bulloch      0            1          1               2   \n",
       "19    Camden      0            0          1               1   \n",
       "20   Candler      1            1          1               1   \n",
       "21   Carroll      0            1          1               3   \n",
       "23  Charlton      1            0          1               0   \n",
       "24   Chatham      0            1          1               7   \n",
       "29      Clay      1            0          1               0   \n",
       "30   Clayton      0            3          4               5   \n",
       "38  Crawford      1            1          0               0   \n",
       "39     Crisp      1            0          1               1   \n",
       "44     Dodge      1            1          1               1   \n",
       "51    Elbert      1            1          1               1   \n",
       "55   Fayette      0            2          2               1   \n",
       "56     Floyd      0            1          1               3   \n",
       "57   Forsyth      0            1          1               1   \n",
       "69   Hancock      1            0          1               0   \n",
       "71    Harris      1            1          1               0   \n",
       "84     Lamar      1            1          0               0   \n",
       "91   Lowndes      0            4          1               2   \n",
       "\n",
       "    dropbox_247availability  dropbox_count    1yrlag    5yrlag   10yrlag  \\\n",
       "5                         0              1 -0.076373 -0.261098  0.231503   \n",
       "7                         1              2  0.039000  0.283471  1.727376   \n",
       "11                        0              1  0.772826  0.772826  0.000000   \n",
       "14                        0              1  0.000000  0.474832  0.474832   \n",
       "15                        0              5 -0.188242 -0.323535  0.352931   \n",
       "19                        0              2  0.201572  0.401834  1.102751   \n",
       "20                        0              1  0.119287  0.000000  0.119287   \n",
       "21                        1              3 -0.000454  0.749205  0.166136   \n",
       "23                        0              1  1.365728  1.365728  0.000000   \n",
       "24                        0              3 -0.065380  0.168274  0.308467   \n",
       "29                        0              2 -0.486449  0.000000 -0.486449   \n",
       "30                        0              0  0.053675 -0.055326  0.521975   \n",
       "38                        1              1 -0.498952 -0.248428 -0.248428   \n",
       "39                        0              1 -0.330353 -0.776784 -0.776784   \n",
       "44                        0              1 -0.231370 -0.231370 -0.385096   \n",
       "51                        0              1  0.513558  2.027116  0.513558   \n",
       "55                        0              2  0.435472  1.870943  0.435472   \n",
       "56                        0              1 -0.115796 -0.115796 -0.167808   \n",
       "57                        0              1  0.540807  1.214909  0.968808   \n",
       "69                        0              0  0.000000  0.000000  0.000000   \n",
       "71                        0              1 -0.480143  0.559572  3.678716   \n",
       "84                        0              1  0.287262  0.930893  0.930893   \n",
       "91                        1              1 -0.073608  0.455760  2.396772   \n",
       "\n",
       "          ...          ER_5yrlag  ER_10yrlag  drug_ER_Visits_2017  \\\n",
       "5         ...           8.221374   16.660819             0.006040   \n",
       "7         ...          13.701365   17.329787             0.008615   \n",
       "11        ...           7.726742    0.000000             0.003377   \n",
       "14        ...          10.824047   11.679245             0.004032   \n",
       "15        ...           9.737864    8.102881             0.002212   \n",
       "19        ...           7.168705    9.075520             0.003496   \n",
       "20        ...          10.980398   10.878870             0.005607   \n",
       "21        ...          14.424508   18.689944             0.007049   \n",
       "23        ...           0.000000    2.643264             0.001920   \n",
       "24        ...          12.468410   10.533582             0.006182   \n",
       "29        ...           0.000000    0.000000             0.000000   \n",
       "30        ...          13.829694   23.608696             0.003396   \n",
       "38        ...          14.703782   18.265464             0.007475   \n",
       "39        ...          19.412979   19.000000             0.006920   \n",
       "44        ...           4.494344   11.026898             0.005665   \n",
       "51        ...           8.326039    3.380267             0.004262   \n",
       "55        ...          13.708155   15.319048             0.003427   \n",
       "56        ...          10.740385   11.943463             0.003663   \n",
       "57        ...           9.804511   12.024169             0.004311   \n",
       "69        ...           9.593248    9.593248             0.006589   \n",
       "71        ...           2.154548    1.512713             0.000580   \n",
       "84        ...          14.010309   13.239609             0.005824   \n",
       "91        ...          11.632353   20.802030             0.004295   \n",
       "\n",
       "    teenage_pregnancy_2017  2017_children_poverty  2017_death_rate  \\\n",
       "5                 0.011871               0.213006         0.008834   \n",
       "7                 0.011750               0.193598         0.008704   \n",
       "11                0.013029               0.333089         0.009503   \n",
       "14                0.005259               0.163188         0.006470   \n",
       "15                0.010492               0.336884         0.007535   \n",
       "19                0.012472               0.212339         0.006585   \n",
       "20                0.014086               0.441832         0.011954   \n",
       "21                0.015083               0.303207         0.009077   \n",
       "23                0.014538               0.403633         0.006443   \n",
       "24                0.010271               0.261827         0.008728   \n",
       "29                0.028434               0.521235         0.011616   \n",
       "30                0.012506               0.369619         0.006306   \n",
       "38                0.013287               0.281088         0.009680   \n",
       "39                0.012496               0.472186         0.010561   \n",
       "44                0.017981               0.387121         0.011531   \n",
       "51                0.015936               0.357183         0.013320   \n",
       "55                0.002559               0.101205         0.008168   \n",
       "56                0.017102               0.321287         0.011752   \n",
       "57                0.001817               0.069888         0.005543   \n",
       "69                0.028099               0.387093         0.015939   \n",
       "71                0.005082               0.122432         0.009296   \n",
       "84                0.008595               0.287860         0.011793   \n",
       "91                0.012464               0.334000         0.007724   \n",
       "\n",
       "    2017_birth_rate  2017_prescription  2017_ODRate  2017ODabovenatavg  \n",
       "5          0.011875           0.386764     0.000198               True  \n",
       "7          0.012125           0.990520     0.000208               True  \n",
       "11         0.009727           1.292364     0.000135              False  \n",
       "14         0.015273           0.898534     0.000040              False  \n",
       "15         0.012205           0.743703     0.000054              False  \n",
       "19         0.014188           0.701366     0.000156              False  \n",
       "20         0.011292           1.788549     0.000103              False  \n",
       "21         0.013053           1.107233     0.000238               True  \n",
       "23         0.009184           1.055230     0.000187               True  \n",
       "24         0.013927           0.816522     0.000112              False  \n",
       "29         0.009518           0.387811     0.000171               True  \n",
       "30         0.014095           0.412495     0.000097              False  \n",
       "38         0.008791           0.013419     0.000122              False  \n",
       "39         0.012637           1.724571     0.000029              False  \n",
       "44         0.009953           0.870092     0.000148              False  \n",
       "51         0.010730           1.475771     0.000159              False  \n",
       "55         0.008228           0.947549     0.000102              False  \n",
       "56         0.011863           1.484394     0.000146              False  \n",
       "57         0.009864           0.479655     0.000154              False  \n",
       "69         0.007573           0.540507     0.000012              False  \n",
       "71         0.008535           0.084966     0.000136              False  \n",
       "84         0.010849           0.735635     0.000207               True  \n",
       "91         0.014248           0.891325     0.000089              False  \n",
       "\n",
       "[23 rows x 42 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risky_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(ok_counties_2018.columns[1:-1])\n",
    "X = ok_counties_2018[features]\n",
    "preds = clf4.predict(X) \n",
    "itemindex = np.where(preds==True)\n",
    "risky_counties = data2018.ix[itemindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>County</th>\n",
       "      <th>Rural</th>\n",
       "      <th>Rehab_Count</th>\n",
       "      <th>EMS_COUNT</th>\n",
       "      <th>hospital_count</th>\n",
       "      <th>dropbox_247availability</th>\n",
       "      <th>dropbox_count</th>\n",
       "      <th>1yrlag</th>\n",
       "      <th>5yrlag</th>\n",
       "      <th>10yrlag</th>\n",
       "      <th>...</th>\n",
       "      <th>ER_5yrlag</th>\n",
       "      <th>ER_10yrlag</th>\n",
       "      <th>drug_ER_Visits_2018</th>\n",
       "      <th>teenage_pregnancy_2018</th>\n",
       "      <th>2018_children_poverty</th>\n",
       "      <th>2018_death_rate</th>\n",
       "      <th>2018_birth_rate</th>\n",
       "      <th>2018_prescription</th>\n",
       "      <th>2018_ODRate</th>\n",
       "      <th>2018ODabovenatavg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Banks</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335376</td>\n",
       "      <td>...</td>\n",
       "      <td>24.883436</td>\n",
       "      <td>24.263473</td>\n",
       "      <td>0.008438</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.195805</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>0.350527</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bartow</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.069945</td>\n",
       "      <td>0.795779</td>\n",
       "      <td>0.459071</td>\n",
       "      <td>...</td>\n",
       "      <td>20.125654</td>\n",
       "      <td>21.926136</td>\n",
       "      <td>0.012105</td>\n",
       "      <td>0.010084</td>\n",
       "      <td>0.184184</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.958487</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bleckley</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060701</td>\n",
       "      <td>-0.059781</td>\n",
       "      <td>0.880438</td>\n",
       "      <td>...</td>\n",
       "      <td>10.874672</td>\n",
       "      <td>10.936358</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.012505</td>\n",
       "      <td>0.340666</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.009435</td>\n",
       "      <td>1.237112</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bryan</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.022556</td>\n",
       "      <td>-0.279217</td>\n",
       "      <td>-0.519478</td>\n",
       "      <td>...</td>\n",
       "      <td>13.303571</td>\n",
       "      <td>15.346939</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.163552</td>\n",
       "      <td>0.006336</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>0.891936</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bulloch</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.145647</td>\n",
       "      <td>-0.566545</td>\n",
       "      <td>-0.133090</td>\n",
       "      <td>...</td>\n",
       "      <td>5.944272</td>\n",
       "      <td>11.530726</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.343252</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.640406</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Butts</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044763</td>\n",
       "      <td>1.719376</td>\n",
       "      <td>0.087750</td>\n",
       "      <td>...</td>\n",
       "      <td>6.200935</td>\n",
       "      <td>5.879464</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.283669</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>1.061031</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Camden</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.048142</td>\n",
       "      <td>1.203982</td>\n",
       "      <td>7.815928</td>\n",
       "      <td>...</td>\n",
       "      <td>7.931441</td>\n",
       "      <td>5.302428</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.217245</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>0.671400</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Candler</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.526910</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.445755</td>\n",
       "      <td>0.012140</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>1.889093</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Charlton</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.045966</td>\n",
       "      <td>-0.381382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.418080</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.009231</td>\n",
       "      <td>1.070460</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Chatham</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.064062</td>\n",
       "      <td>0.122815</td>\n",
       "      <td>0.740364</td>\n",
       "      <td>...</td>\n",
       "      <td>16.607724</td>\n",
       "      <td>14.580935</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.266240</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.784477</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Clay</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.102629</td>\n",
       "      <td>-0.433743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029274</td>\n",
       "      <td>0.511604</td>\n",
       "      <td>0.011758</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.316620</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Clayton</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.167037</td>\n",
       "      <td>0.556049</td>\n",
       "      <td>...</td>\n",
       "      <td>26.162921</td>\n",
       "      <td>19.400844</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.377429</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>0.377810</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Coffee</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042117</td>\n",
       "      <td>0.867203</td>\n",
       "      <td>0.867203</td>\n",
       "      <td>...</td>\n",
       "      <td>7.843182</td>\n",
       "      <td>5.023220</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.301762</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>1.770039</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Crawford</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.074905</td>\n",
       "      <td>0.390551</td>\n",
       "      <td>0.390551</td>\n",
       "      <td>...</td>\n",
       "      <td>17.725000</td>\n",
       "      <td>15.995138</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>0.267932</td>\n",
       "      <td>0.009837</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>0.015839</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Crisp</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.352954</td>\n",
       "      <td>-0.566707</td>\n",
       "      <td>-0.855569</td>\n",
       "      <td>...</td>\n",
       "      <td>21.515152</td>\n",
       "      <td>23.958656</td>\n",
       "      <td>0.009659</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.471644</td>\n",
       "      <td>0.010531</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>1.691126</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Dodge</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047579</td>\n",
       "      <td>2.220800</td>\n",
       "      <td>-0.355840</td>\n",
       "      <td>...</td>\n",
       "      <td>15.516641</td>\n",
       "      <td>14.134899</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.391279</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.009539</td>\n",
       "      <td>0.804906</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Elbert</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.022887</td>\n",
       "      <td>-0.408433</td>\n",
       "      <td>-0.577452</td>\n",
       "      <td>...</td>\n",
       "      <td>11.756303</td>\n",
       "      <td>4.486747</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.363121</td>\n",
       "      <td>0.013468</td>\n",
       "      <td>0.010495</td>\n",
       "      <td>1.484541</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Fayette</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>0.449407</td>\n",
       "      <td>0.656465</td>\n",
       "      <td>...</td>\n",
       "      <td>19.083333</td>\n",
       "      <td>20.140351</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.101808</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.977098</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Floyd</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.003382</td>\n",
       "      <td>-0.118786</td>\n",
       "      <td>-0.118786</td>\n",
       "      <td>...</td>\n",
       "      <td>10.206587</td>\n",
       "      <td>16.909091</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.017696</td>\n",
       "      <td>0.327025</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.011612</td>\n",
       "      <td>1.427174</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Forsyth</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028522</td>\n",
       "      <td>0.349975</td>\n",
       "      <td>1.024963</td>\n",
       "      <td>...</td>\n",
       "      <td>11.440748</td>\n",
       "      <td>21.581132</td>\n",
       "      <td>0.005984</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.069207</td>\n",
       "      <td>0.005546</td>\n",
       "      <td>0.009162</td>\n",
       "      <td>0.451561</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Habersham</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.016215</td>\n",
       "      <td>0.035155</td>\n",
       "      <td>-0.171876</td>\n",
       "      <td>...</td>\n",
       "      <td>5.529582</td>\n",
       "      <td>8.159919</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>0.229887</td>\n",
       "      <td>0.010203</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.870900</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Hancock</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.898986</td>\n",
       "      <td>...</td>\n",
       "      <td>14.231511</td>\n",
       "      <td>14.231511</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.364671</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.536308</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080487</td>\n",
       "      <td>-0.157452</td>\n",
       "      <td>4.055291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663315</td>\n",
       "      <td>1.347834</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.115648</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.008485</td>\n",
       "      <td>0.087579</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Lamar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.102059</td>\n",
       "      <td>3.255914</td>\n",
       "      <td>0.418638</td>\n",
       "      <td>...</td>\n",
       "      <td>22.688623</td>\n",
       "      <td>11.970492</td>\n",
       "      <td>0.007912</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.284600</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.715270</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Lee</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.126033</td>\n",
       "      <td>-0.426400</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572043</td>\n",
       "      <td>1.024880</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.176522</td>\n",
       "      <td>0.007923</td>\n",
       "      <td>0.010973</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Lowndes</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018963</td>\n",
       "      <td>1.076711</td>\n",
       "      <td>1.595889</td>\n",
       "      <td>...</td>\n",
       "      <td>13.837093</td>\n",
       "      <td>46.741935</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.007853</td>\n",
       "      <td>0.014441</td>\n",
       "      <td>0.855649</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       County  Rural  Rehab_Count  EMS_COUNT  hospital_count  \\\n",
       "5       Banks      1            0          1               0   \n",
       "7      Bartow      0            1          1               1   \n",
       "11   Bleckley      1            1          1               1   \n",
       "14      Bryan      1            0          1               0   \n",
       "15    Bulloch      0            1          1               2   \n",
       "17      Butts      1            1          1               1   \n",
       "19     Camden      0            0          1               1   \n",
       "20    Candler      1            1          1               1   \n",
       "23   Charlton      1            0          1               0   \n",
       "24    Chatham      0            1          1               7   \n",
       "29       Clay      1            0          1               0   \n",
       "30    Clayton      0            3          4               5   \n",
       "33     Coffee      0            1          1               1   \n",
       "38   Crawford      1            1          0               0   \n",
       "39      Crisp      1            0          1               1   \n",
       "44      Dodge      1            1          1               1   \n",
       "51     Elbert      1            1          1               1   \n",
       "55    Fayette      0            2          2               1   \n",
       "56      Floyd      0            1          1               3   \n",
       "57    Forsyth      0            1          1               1   \n",
       "67  Habersham      0            2          1               1   \n",
       "69    Hancock      1            0          1               0   \n",
       "71     Harris      1            1          1               0   \n",
       "84      Lamar      1            1          0               0   \n",
       "87        Lee      1            1          1               0   \n",
       "91    Lowndes      0            4          1               2   \n",
       "\n",
       "    dropbox_247availability  dropbox_count    1yrlag    5yrlag   10yrlag  \\\n",
       "5                         0              1  0.084346  0.000000  0.335376   \n",
       "7                         1              2  0.069945  0.795779  0.459071   \n",
       "11                        0              1  0.060701 -0.059781  0.880438   \n",
       "14                        0              1 -0.022556 -0.279217 -0.519478   \n",
       "15                        0              5 -0.145647 -0.566545 -0.133090   \n",
       "17                        1              1  0.044763  1.719376  0.087750   \n",
       "19                        0              2  0.048142  1.203982  7.815928   \n",
       "20                        0              1  0.013400  0.000000  0.000000   \n",
       "23                        0              1  0.045966 -0.381382  0.000000   \n",
       "24                        0              3  0.064062  0.122815  0.740364   \n",
       "29                        0              2  0.102629 -0.433743  0.000000   \n",
       "30                        0              0  0.022388  0.167037  0.556049   \n",
       "33                        0              1  0.042117  0.867203  0.867203   \n",
       "38                        1              1 -0.074905  0.390551  0.390551   \n",
       "39                        0              1 -0.352954 -0.566707 -0.855569   \n",
       "44                        0              1  0.047579  2.220800 -0.355840   \n",
       "51                        0              1 -0.022887 -0.408433 -0.577452   \n",
       "55                        0              2  0.009708  0.449407  0.656465   \n",
       "56                        0              1 -0.003382 -0.118786 -0.118786   \n",
       "57                        0              1  0.028522  0.349975  1.024963   \n",
       "67                        1              1 -0.016215  0.035155 -0.171876   \n",
       "69                        0              0  0.000000  0.000000 -0.898986   \n",
       "71                        0              1  0.080487 -0.157452  4.055291   \n",
       "84                        0              1  0.102059  3.255914  0.418638   \n",
       "87                        0              1 -0.126033 -0.426400  0.147200   \n",
       "91                        1              1  0.018963  1.076711  1.595889   \n",
       "\n",
       "          ...          ER_5yrlag  ER_10yrlag  drug_ER_Visits_2018  \\\n",
       "5         ...          24.883436   24.263473             0.008438   \n",
       "7         ...          20.125654   21.926136             0.012105   \n",
       "11        ...          10.874672   10.936358             0.004595   \n",
       "14        ...          13.303571   15.346939             0.005607   \n",
       "15        ...           5.944272   11.530726             0.002243   \n",
       "17        ...           6.200935    5.879464             0.004623   \n",
       "19        ...           7.931441    5.302428             0.004689   \n",
       "20        ...           0.000000   15.526910             0.007735   \n",
       "23        ...           0.000000    0.000000             0.001920   \n",
       "24        ...          16.607724   14.580935             0.008663   \n",
       "29        ...           0.000000    0.000000             0.000000   \n",
       "30        ...          26.162921   19.400844             0.004835   \n",
       "33        ...           7.843182    5.023220             0.003891   \n",
       "38        ...          17.725000   15.995138             0.010486   \n",
       "39        ...          21.515152   23.958656             0.009659   \n",
       "44        ...          15.516641   14.134899             0.007779   \n",
       "51        ...          11.756303    4.486747             0.004554   \n",
       "55        ...          19.083333   20.140351             0.004820   \n",
       "56        ...          10.206587   16.909091             0.003743   \n",
       "57        ...          11.440748   21.581132             0.005984   \n",
       "67        ...           5.529582    8.159919             0.004525   \n",
       "69        ...          14.231511   14.231511             0.009474   \n",
       "71        ...           0.663315    1.347834             0.000610   \n",
       "84        ...          22.688623   11.970492             0.007912   \n",
       "87        ...           0.572043    1.024880             0.000865   \n",
       "91        ...          13.837093   46.741935             0.005920   \n",
       "\n",
       "    teenage_pregnancy_2018  2018_children_poverty  2018_death_rate  \\\n",
       "5                 0.010746               0.195805         0.009037   \n",
       "7                 0.010084               0.184184         0.008822   \n",
       "11                0.012505               0.340666         0.009164   \n",
       "14                0.003983               0.163552         0.006336   \n",
       "15                0.009892               0.343252         0.007868   \n",
       "17                0.009128               0.283669         0.010469   \n",
       "19                0.011173               0.217245         0.006708   \n",
       "20                0.011502               0.445755         0.012140   \n",
       "23                0.012982               0.418080         0.006179   \n",
       "24                0.008751               0.266240         0.008844   \n",
       "29                0.029274               0.511604         0.011758   \n",
       "30                0.010712               0.377429         0.006470   \n",
       "33                0.016591               0.301762         0.009536   \n",
       "38                0.012252               0.267932         0.009837   \n",
       "39                0.006945               0.471644         0.010531   \n",
       "44                0.014674               0.391279         0.011667   \n",
       "51                0.014269               0.363121         0.013468   \n",
       "55                0.002266               0.101808         0.008444   \n",
       "56                0.017696               0.327025         0.012091   \n",
       "57                0.001034               0.069207         0.005546   \n",
       "67                0.010520               0.229887         0.010203   \n",
       "69                0.029000               0.364671         0.017860   \n",
       "71                0.003785               0.115648         0.009351   \n",
       "84                0.007032               0.284600         0.011961   \n",
       "87                0.010459               0.176522         0.007923   \n",
       "91                0.011030               0.340000         0.007853   \n",
       "\n",
       "    2018_birth_rate  2018_prescription  2018_ODRate  2018ODabovenatavg  \n",
       "5          0.011996           0.350527     0.000212               True  \n",
       "7          0.011669           0.958487     0.000221               True  \n",
       "11         0.009435           1.237112     0.000142              False  \n",
       "14         0.015284           0.891936     0.000038              False  \n",
       "15         0.012269           0.640406     0.000045              False  \n",
       "17         0.009958           1.061031     0.000224               True  \n",
       "19         0.013843           0.671400     0.000162              False  \n",
       "20         0.010810           1.889093     0.000104              False  \n",
       "23         0.009231           1.070460     0.000193               True  \n",
       "24         0.013752           0.784477     0.000118              False  \n",
       "29         0.008958           0.316620     0.000189               True  \n",
       "30         0.013683           0.377810     0.000097              False  \n",
       "33         0.012926           1.770039     0.000173               True  \n",
       "38         0.008416           0.015839     0.000113              False  \n",
       "39         0.012459           1.691126     0.000019              False  \n",
       "44         0.009539           0.804906     0.000154              False  \n",
       "51         0.010495           1.484541     0.000157              False  \n",
       "55         0.008218           0.977098     0.000102              False  \n",
       "56         0.011612           1.427174     0.000145              False  \n",
       "57         0.009162           0.451561     0.000153              False  \n",
       "67         0.010870           0.870900     0.000092              False  \n",
       "69         0.007364           0.536308     0.000012              False  \n",
       "71         0.008485           0.087579     0.000145              False  \n",
       "84         0.010811           0.715270     0.000227               True  \n",
       "87         0.010973           0.546000     0.000078              False  \n",
       "91         0.014441           0.855649     0.000090              False  \n",
       "\n",
       "[26 rows x 42 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risky_counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
